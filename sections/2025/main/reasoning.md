# ICML-2025-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICML-2025-Papers/blob/main/sections/2025/main/optimal-transport.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICML-2025-Papers/blob/main/README.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICML-2025-Papers/blob/main/sections/2025/main/representations.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Reasoning

![Section Papers](https://img.shields.io/badge/Section%20Papers-4-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-4-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-4-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-0-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| [Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark](https://icml.cc/virtual/2025/poster/43702) | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://emma-benchmark.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/EMMA-Bench/EMMA?style=flat)](https://github.com/EMMA-Bench/EMMA) <br /> [![Hugging Face Dataset](https://img.shields.io/badge/ðŸ¤—-dataset-FFD21F.svg)](https://huggingface.co/datasets/luckychao/EMMA) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/43702) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2501.05444-b31b1b.svg)](http://arxiv.org/abs/2501.05444) | :heavy_minus_sign: |
| [Roll the Dice and Look before You Leap: Going beyond the Creative Limits of Next-Token Prediction](https://icml.cc/virtual/2025/poster/45769) | [![GitHub](https://img.shields.io/github/stars/ChenWu98/algorithmic-creativity?style=flat)](https://github.com/ChenWu98/algorithmic-creativity) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/45769) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2504.15266-b31b1b.svg)](http://arxiv.org/abs/2504.15266) | :heavy_minus_sign: |
| [rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking](https://icml.cc/virtual/2025/poster/46400) | [![GitHub](https://img.shields.io/github/stars/microsoft/rStar?style=flat)](https://github.com/microsoft/rStar) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/46400) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2501.04519-b31b1b.svg)](http://arxiv.org/abs/2501.04519) | :heavy_minus_sign: |
| [VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data](https://icml.cc/virtual/2025/poster/44223) | [![GitHub](https://img.shields.io/github/stars/UW-Madison-Lee-Lab/VersaPRM?style=flat)](https://github.com/UW-Madison-Lee-Lab/VersaPRM) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/44223) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2502.06737-b31b1b.svg)](http://arxiv.org/abs/2502.06737) | :heavy_minus_sign: |
