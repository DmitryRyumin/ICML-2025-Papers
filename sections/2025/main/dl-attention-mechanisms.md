# ICML-2025-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICML-2025-Papers/blob/main/sections/2025/main/dl-algorithms.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICML-2025-Papers/blob/main/README.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICML-2025-Papers/blob/main/sections/2025/main/dl-everything-else.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Deep Learning (Attention Mechanisms)

![Section Papers](https://img.shields.io/badge/Section%20Papers-36-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-0-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-0-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-0-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| [LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models](https://icml.cc/virtual/2025/poster/44230) | [![GitHub](https://img.shields.io/github/stars/uw-mad-dash/LV-XAttn?style=flat)](https://github.com/uw-mad-dash/LV-XAttn) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/44230) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2502.02406-b31b1b.svg)](http://arxiv.org/abs/2502.02406) | :heavy_minus_sign: |
| [SpargeAttention: Accurate and Training-Free Sparse Attention Accelerating any Model Inference](https://icml.cc/virtual/2025/poster/46341) | [![GitHub](https://img.shields.io/github/stars/thu-ml/SpargeAttn?style=flat)](https://github.com/thu-ml/SpargeAttn) <br /> [![Hugging Face Model](https://img.shields.io/badge/ðŸ¤—-model-FFD21F.svg)](https://huggingface.co/Xiang-cd/sparge-attention-model-zoo) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/46341) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2502.18137-b31b1b.svg)](http://arxiv.org/abs/2502.18137) | :heavy_minus_sign: |
| [MoH: Multi-Head Attention as Mixture-of-Head Attention](https://icml.cc/virtual/2025/poster/44579) | [![GitHub](https://img.shields.io/github/stars/SkyworkAI/MoH?style=flat)](https://github.com/SkyworkAI/MoH) <br /> [![Hugging Face Collection](https://img.shields.io/badge/ðŸ¤—-collection-FFD21F.svg)](https://huggingface.co/collections/Chat-UniVi/moh-66f4277375c1c1b2ad61a2c1) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/44579) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2410.11842-b31b1b.svg)](http://arxiv.org/abs/2410.11842) | :heavy_minus_sign: |
| [RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers](https://icml.cc/virtual/2025/poster/44361) | [![GitHub](https://img.shields.io/github/stars/Ackesnal/RePaViT?style=flat)](https://github.com/Ackesnal/RePaViT) <br /> [![Hugging Face Model](https://img.shields.io/badge/ðŸ¤—-model-FFD21F.svg)](https://huggingface.co/Ackesnal/RePaViT/tree/main) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/44361) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2505.21847-b31b1b.svg)](http://arxiv.org/abs/2505.21847) | :heavy_minus_sign: |
| [Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion](https://icml.cc/virtual/2025/poster/43634) | [![GitHub](https://img.shields.io/github/stars/zhengzaiyi/RotationSymmetry?style=flat)](https://github.com/zhengzaiyi/RotationSymmetry) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/43634) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2502.00264-b31b1b.svg)](http://arxiv.org/abs/2502.00264) | :heavy_minus_sign: |
| [Hgformer: Hyperbolic Graph Transformer for Collaborative Filtering](https://icml.cc/virtual/2025/poster/44203) | [![GitHub](https://img.shields.io/github/stars/EnkiXin/Hgformer?style=flat)](https://github.com/EnkiXin/Hgformer) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/44203) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2502.15693-b31b1b.svg)](http://arxiv.org/abs/2502.15693) | :heavy_minus_sign: |
| [A Theoretical Study of (Hyper) Self-Attention through the Lens of Interactions: Representation, Training, Generalization](https://icml.cc/virtual/2025/poster/43622) | :heavy_minus_sign: | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/43622) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2506.06179-b31b1b.svg)](http://arxiv.org/abs/2506.06179) | :heavy_minus_sign: |
| [How Transformers Learn Structured Data: Insights from Hierarchical Filtering](https://icml.cc/virtual/2025/poster/46167) | [![GitHub](https://img.shields.io/github/stars/emanuele-moscato/tree-language-paper-submission?style=flat)](https://github.com/emanuele-moscato/tree-language-paper-submission) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/46167) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2408.15138-b31b1b.svg)](http://arxiv.org/abs/2408.15138) | :heavy_minus_sign: |
| [Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs](https://icml.cc/virtual/2025/poster/46284) | [![GitHub](https://img.shields.io/github/stars/agbrothers/pooling?style=flat)](https://github.com/agbrothers/pooling) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/46284) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2506.09215-b31b1b.svg)](http://arxiv.org/abs/2506.09215) | :heavy_minus_sign: |
| [EPIC: Efficient Position-Independent Caching for Serving Large Language Models](https://icml.cc/virtual/2025/poster/43926) | [![GitHub](https://img.shields.io/github/stars/DerekHJH/epic?style=flat)](https://github.com/DerekHJH/epic) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/43926) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2410.15332-b31b1b.svg)](http://arxiv.org/abs/2410.15332) | :heavy_minus_sign: |
| [On Expressive Power of Looped Transformers: Theoretical Analysis and Enhancement via Timestep Encoding](https://icml.cc/virtual/2025/poster/45800) | [![GitHub](https://img.shields.io/github/stars/kevin671/tmlt?style=flat)](https://github.com/kevin671/tmlt) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/45800) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2410.01405-b31b1b.svg)](http://arxiv.org/abs/2410.01405) | :heavy_minus_sign: |
| [On Understanding Attention-based In-Context Learning for Categorical Data](https://icml.cc/virtual/2025/poster/46334) | [![GitHub](https://img.shields.io/github/stars/aarontwang/icl_attention_categorical?style=flat)](https://github.com/aarontwang/icl_attention_categorical) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/46334) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2405.17248-b31b1b.svg)](http://arxiv.org/abs/2405.17248) | :heavy_minus_sign: |
| [Curvature-aware Graph Attention for PDEs on Manifolds](https://icml.cc/virtual/2025/poster/43668) | [![GitHub](https://img.shields.io/github/stars/Supradax/CurvGT?style=flat)](https://github.com/Supradax/CurvGT) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/43668) | :heavy_minus_sign: |
| [Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling](https://icml.cc/virtual/2025/poster/46384) | [![GitHub](https://img.shields.io/github/stars/ant-research/long-context-modeling?style=flat)](https://github.com/ant-research/long-context-modeling) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/46384) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2410.01651-b31b1b.svg)](http://arxiv.org/abs/2410.01651) | :heavy_minus_sign: |
| [Token Coordinated Prompt Attention is Needed for Visual Prompting](https://icml.cc/virtual/2025/poster/45424) | [![GitHub](https://img.shields.io/github/stars/zhoujiahuan1991/ICML2025-TCPA?style=flat)](https://github.com/zhoujiahuan1991/ICML2025-TCPA) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/45424) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2505.02406-b31b1b.svg)](http://arxiv.org/abs/2505.02406) | :heavy_minus_sign: |
| [Primphormer: Efficient Graph Transformers with Primal Representations](https://icml.cc/virtual/2025/poster/44528) | :heavy_minus_sign: | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/44528) | :heavy_minus_sign: |
| [Nonparametric Modern Hopfield Models](https://icml.cc/virtual/2025/poster/43568) | [![GitHub](https://img.shields.io/github/stars/MAGICS-LAB/NonparametricHopfield?style=flat)](https://github.com/MAGICS-LAB/NonparametricHopfield) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/43568) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2404.03900-b31b1b.svg)](http://arxiv.org/abs/2404.03900) | :heavy_minus_sign: |
| [MVA: Linear Attention with High-Order Query-Keys Integration and Multi-Level Vocabulary Decomposition](https://icml.cc/virtual/2025/poster/45016) | :heavy_minus_sign: | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/45016) | :heavy_minus_sign: |
| [ParallelComp: Parallel Long-Context Compressor for Length Extrapolation](https://icml.cc/virtual/2025/poster/46101) | [![GitHub](https://img.shields.io/github/stars/menik1126/ParallelComp?style=flat)](https://github.com/menik1126/ParallelComp) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/46101) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2502.14317-b31b1b.svg)](http://arxiv.org/abs/2502.14317) | :heavy_minus_sign: |
| [Sparse Video-Gen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity](https://icml.cc/virtual/2025/poster/43743) | [![GitHub](https://img.shields.io/github/stars/svg-project/Sparse-VideoGen?style=flat)](https://github.com/svg-project/Sparse-VideoGen) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/43743) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2502.01776-b31b1b.svg)](http://arxiv.org/abs/2502.01776) | :heavy_minus_sign: |
| [Attention-Level Speculation](https://icml.cc/virtual/2025/poster/46486) | :heavy_minus_sign: | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/46486) | :heavy_minus_sign: |
| [Benign Overfitting in Token Selection of Attention Mechanism](https://icml.cc/virtual/2025/poster/45799) | [![GitHub](https://img.shields.io/github/stars/keitaroskmt/benign-attention?style=flat)](https://github.com/keitaroskmt/benign-attention) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/45799) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2409.17625-b31b1b.svg)](http://arxiv.org/abs/2409.17625) | :heavy_minus_sign: |
| [âˆž-Video: A Training-Free Approach to Long Video Understanding via Continuous-Time Memory Consolidation](https://icml.cc/virtual/2025/poster/44785) | [![GitHub](https://img.shields.io/github/stars/deep-spin/Infinite-Video?style=flat)](https://github.com/deep-spin/Infinite-Video) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/44785) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2501.19098-b31b1b.svg)](http://arxiv.org/abs/2501.19098) | :heavy_minus_sign: |
| [ESPFormer: Doubly-Stochastic Attention with Expected Sliced Transport Plans](https://icml.cc/virtual/2025/poster/45096) | [![GitHub](https://img.shields.io/github/stars/dariansal/ESPFormer?style=flat)](https://github.com/dariansal/ESPFormer) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/45096) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2502.07962-b31b1b.svg)](http://arxiv.org/abs/2502.07962) | :heavy_minus_sign: |
| [Customizing the Inductive Biases of Softmax Attention using Structured Matrices](https://icml.cc/virtual/2025/poster/45261) | [![GitHub](https://img.shields.io/github/stars/YilunKuang/structured-attention?style=flat)](https://github.com/YilunKuang/structured-attention) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/45261) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2509.07963-b31b1b.svg)](http://arxiv.org/abs/2509.07963) | :heavy_minus_sign: |
| [DeepCrossAttention: Supercharging Transformer Residual Connections](https://icml.cc/virtual/2025/poster/44325) | [![GitHub](https://img.shields.io/github/stars/lucidrains/deep-cross-attention?style=flat)](https://github.com/lucidrains/deep-cross-attention) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/44325) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2502.06785-b31b1b.svg)](http://arxiv.org/abs/2502.06785) | :heavy_minus_sign: |
| [Selective Prompt Anchoring for Code Generation](https://icml.cc/virtual/2025/poster/44812) | [![GitHub](https://img.shields.io/github/stars/magic-YuanTian/Selective-Prompt-Anchoring?style=flat)](https://github.com/magic-YuanTian/Selective-Prompt-Anchoring) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/44812) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2408.09121-b31b1b.svg)](http://arxiv.org/abs/2408.09121) | :heavy_minus_sign: |
| [The Underlying Structures of Self-Attention: Symmetry, Directionality, and Emergent Dynamics in Transformer Training](https://icml.cc/virtual/2025/poster/44452) | [![GitHub](https://img.shields.io/github/stars/matteosaponati/attention-geometry?style=flat)](https://github.com/matteosaponati/attention-geometry) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/44452) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2502.10927-b31b1b.svg)](http://arxiv.org/abs/2502.10927) | :heavy_minus_sign: |
| [On the Emergence of Position Bias in Transformers](https://icml.cc/virtual/2025/poster/44889) | [![GitHub](https://img.shields.io/github/stars/xinyiwu98/position-bias-in-attention?style=flat)](https://github.com/xinyiwu98/position-bias-in-attention) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/44889) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2502.01951-b31b1b.svg)](http://arxiv.org/abs/2502.01951) | :heavy_minus_sign: |
| [HashAttention: Semantic Sparsity for Faster Inference](https://icml.cc/virtual/2025/poster/45928) | [![GitHub](https://img.shields.io/github/stars/xAlg-ai/HashAttention-1.0?style=flat)](https://github.com/xAlg-ai/HashAttention-1.0) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/45928) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2412.14468-b31b1b.svg)](http://arxiv.org/abs/2412.14468) | :heavy_minus_sign: |
| [LASER: Attention with Exponential Transformation](https://icml.cc/virtual/2025/poster/44345) | [![GitHub](https://img.shields.io/github/stars/lucidrains/transfusion-pytorch?style=flat)](https://github.com/lucidrains/transfusion-pytorch) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/44345) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2411.03493-b31b1b.svg)](http://arxiv.org/abs/2411.03493) | :heavy_minus_sign: |
| [SageAttention2: Efficient Attention with thorough Outlier Smoothing and Per-thread INT4 Quantization](https://icml.cc/virtual/2025/poster/44114) | [![GitHub](https://img.shields.io/github/stars/thu-ml/SageAttention?style=flat)](https://github.com/thu-ml/SageAttention) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/44114) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2411.10958-b31b1b.svg)](http://arxiv.org/abs/2411.10958) | :heavy_minus_sign: |
| [Disentangling and Integrating Relational and Sensory Information in Transformer Architectures](https://icml.cc/virtual/2025/poster/44191) | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://awni.xyz/dual-attention/) <br /> [![GitHub](https://img.shields.io/github/stars/Awni00/dual-attention?style=flat)](https://github.com/Awni00/dual-attention) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/44191) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2405.16727-b31b1b.svg)](http://arxiv.org/abs/2405.16727) | :heavy_minus_sign: |
| [Visual Attention Never Fades: Selective Progressive Attention ReCalibration for Detailed Image Captioning in Multimodal Large Language Models](https://icml.cc/virtual/2025/poster/43450) | [![GitHub](https://img.shields.io/github/stars/mingi000508/SPARC?style=flat)](https://github.com/mingi000508/SPARC) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/43450) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2502.01419-b31b1b.svg)](http://arxiv.org/abs/2502.01419) | :heavy_minus_sign: |
| [In-Context Linear Regression Demystified: Training Dynamics and Mechanistic Interpretability of Multi-Head Softmax Attention](https://icml.cc/virtual/2025/poster/46533) | [![GitHub](https://img.shields.io/github/stars/Y-Agent/ICL_linear?style=flat)](https://github.com/Y-Agent/ICL_linear) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/46533) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2503.12734-b31b1b.svg)](http://arxiv.org/abs/2503.12734) | :heavy_minus_sign: |
| [Star Attention: Efficient LLM Inference over Long Sequences](https://icml.cc/virtual/2025/poster/45335) | [![GitHub](https://img.shields.io/github/stars/NVIDIA/Star-Attention?style=flat)](https://github.com/NVIDIA/Star-Attention) | [![icml.cc](https://img.shields.io/badge/html-icml.cc-2494E0.svg)](https://icml.cc/virtual/2025/poster/45335) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2411.17116-b31b1b.svg)](http://arxiv.org/abs/2411.17116) | :heavy_minus_sign: |
